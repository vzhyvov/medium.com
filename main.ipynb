{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!../venv/Scripts/python.exe\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from fake_useragent import UserAgent\n",
    "from constants import Const as const\n",
    "\n",
    "\n",
    "class App:\n",
    "    def __init__(self):\n",
    "        start_url = self.check_files_and_start()\n",
    "        print(\"start ->\", start_url)\n",
    "        parser = Parser(start_url)\n",
    "        \n",
    "    def check_files_and_start(self):\n",
    "        with open(const.RESERVE_URLS_PATH, \"r\") as reserve_urls_file: # Working with files\n",
    "            reserve_urls_text = reserve_urls_file.read()\n",
    "\n",
    "            if reserve_urls_text:\n",
    "                return random.choice(reserve_urls_text.split(\"\\n\"))\n",
    "            else:\n",
    "                with open(const.USED_URLS_PATH, \"r\") as used_urls_file:\n",
    "                    used_urls_text = used_urls_file.read()\n",
    "\n",
    "                if not used_urls_text:\n",
    "                    with open(const.OUT_PATH, \"w\", encoding=const.CSV_ENCODING, newline=\"\") as csv_file:\n",
    "                        field_names = const.DEFAULT_FIELDS\n",
    "                        writer = csv.DictWriter(csv_file, fieldnames=field_names)\n",
    "                        writer.writeheader()\n",
    "\n",
    "                with open(const.USED_URLS_PATH, \"r\") as used_urls_file:\n",
    "                    last_used_url = used_urls_file.read().split(\"\\n\")[-3]\n",
    "\n",
    "                return last_used_url\n",
    "\n",
    "class Parser:\n",
    "    def __init__(self, start):\n",
    "        print(\"collecting User-Agents...\")\n",
    "        self.fake_ua = UserAgent()\n",
    "        self.user_agents = self.get_user_agents_list()  # Collecting User-Agents\n",
    "        print(\"done\")\n",
    "        \n",
    "        self.pool = ThreadPool(const.THREADS_NUMBER)  # Creating Pool object of multiprocessing based on threads\n",
    "\n",
    "        self.current_urls = self.get_basic_urls(start)  # Collecting basic list of urls\n",
    "        print(self.current_urls)\n",
    "\n",
    "        self.new_urls = []  # Extra list is necessary in the code\n",
    "\n",
    "        with open(const.USED_URLS_PATH, \"r\") as used_urls_file:  # Reading used urls from .txt file\n",
    "            self.used_urls = used_urls_file.read().split(\"\\n\")\n",
    "        self.counter = 0\n",
    "\n",
    "        while True:  # Main loop\n",
    "            self.main_loop_iteration()  # Main function of the loop\n",
    "\n",
    "    def get_user_agents_list(self):  # Loading User-Agents from .json file\n",
    "        json_file_data = json.load(open(const.JSON_UA_PATH))\n",
    "        user_agents_list = []\n",
    "        for piece_of_data in json_file_data[\"useragentswitcher\"][\"folder\"]:\n",
    "            try:\n",
    "                for user_agent in piece_of_data[\"useragent\"]:\n",
    "                    user_agents_list.append(user_agent[\"-useragent\"])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        return user_agents_list\n",
    "\n",
    "    def get_basic_urls(self, start_url):\n",
    "        basic_urls = self.get_followers_or_following(\n",
    "            start_url + \"/following\",\n",
    "            self.get_html(start_url + \"/following\")\n",
    "        )\n",
    "        basic_urls.extend(\n",
    "            self.get_followers_or_following(\n",
    "                start_url + \"/followers\",\n",
    "                self.get_html(start_url + \"/followers\")\n",
    "            )\n",
    "        )\n",
    "        return basic_urls\n",
    "\n",
    "    def get_followers_or_following(self, url_to_followers, html): \n",
    "        # Get new urls from following and followers\n",
    "        only_a_tags = SoupStrainer(const.A_TAG)\n",
    "        a_soup = BeautifulSoup(\n",
    "            html, \n",
    "            \"lxml\", \n",
    "            parse_only=only_a_tags\n",
    "        )\n",
    "        followers_links_list = [\n",
    "            link.get(\"href\") for link in a_soup.find_all(\n",
    "                const.A_TAG,\n",
    "                {\"class\": const.A_FOLLOW_CLASS}\n",
    "            )\n",
    "        ]\n",
    "        return followers_links_list\n",
    "\n",
    "    def get_html(self, url_to_html):  # Request -> response -> html\n",
    "        try:\n",
    "            html_code = requests.get(\n",
    "                url=url_to_html,\n",
    "                headers={\"User-Agent\": self.fake_ua.random}\n",
    "            ).text\n",
    "        except:\n",
    "            try:\n",
    "                html_code = requests.get(\n",
    "                    url=url_to_html,\n",
    "                    headers={\"User-Agent\": random.choice(self.user_agents)}\n",
    "                ).text\n",
    "                return html_code\n",
    "            except:\n",
    "                return self.get_html(url_to_html)\n",
    "        else:\n",
    "            return html_code\n",
    "    \n",
    "    def main_loop_iteration(self):\n",
    "        self.pool.map(self.parse, self.current_urls)  # Using multiprocessing for current urls\n",
    "\n",
    "        self.current_urls = self.new_urls  # Replacing current urls with new urls\n",
    "\n",
    "        with open(const.USED_URLS_PATH, \"w\") as used_urls_file:  # Write new used urls to .txt file\n",
    "            used_urls_file.write(\"\\n\".join(self.used_urls))\n",
    "        with open(const.RESERVE_URLS_PATH, \"w\") as reserve_urls_file:  # Write reserve urls to .txt file\n",
    "            reserve_urls_file.write(\"\\n\".join(self.current_urls))\n",
    "\n",
    "        print(\"Current urls ->\", len(self.current_urls))  # Print number of available urls\n",
    "\n",
    "        time.sleep(const.ITERATION_TIMEOUT)  # Iteration timeout (default = 3s)\n",
    "    \n",
    "    def parse(self, profile_url):\n",
    "        if profile_url in self.used_urls:\n",
    "            print(\"used\")  # Checking url\n",
    "            return\n",
    "        else:\n",
    "            self.used_urls.append(profile_url)  # New used url\n",
    "\n",
    "        with open(const.USED_URLS_PATH, \"a\") as used_urls_file:\n",
    "            used_urls_file.write(profile_url + \"\\n\")\n",
    "\n",
    "        following_url = profile_url + \"/following\"\n",
    "        followers_url = profile_url + \"/followers\"\n",
    "\n",
    "        html_of_user = self.get_html(profile_url)  # Get html code of user, following & followers pages\n",
    "        html_of_following = self.get_html(following_url)\n",
    "        html_of_followers = self.get_html(followers_url)\n",
    "\n",
    "        if html_of_user is None:\n",
    "            return\n",
    "\n",
    "        only_div_tags = SoupStrainer(const.DIV_TAG)  # Generating tag keys for a beautiful soup\n",
    "        only_link_tags = SoupStrainer(const.A_TAG)\n",
    "\n",
    "        header_soup = BeautifulSoup(  # Generating BeautifulSoup object\n",
    "            html_of_user,\n",
    "            \"lxml\",\n",
    "            parse_only=only_div_tags\n",
    "        )\n",
    "        buttonset_soup_following = BeautifulSoup(\n",
    "            html_of_following,\n",
    "            \"lxml\",\n",
    "            parse_only=only_link_tags\n",
    "        )\n",
    "        buttonset_soup_followers = BeautifulSoup(\n",
    "            html_of_followers,\n",
    "            \"lxml\",\n",
    "            parse_only=only_link_tags\n",
    "        )\n",
    "        \n",
    "        hero_name = self.get_user_name(header_soup, profile_url)\n",
    "        \n",
    "        hero_description = self.get_user_description(header_soup)\n",
    "\n",
    "        following_people_quantity = self.get_number_of_following(buttonset_soup_following)\n",
    "        followers_quantity = self.get_number_of_followers(buttonset_soup_following)\n",
    "\n",
    "        twitter_url = self.get_twitter_url(buttonset_soup_following)\n",
    "        facebook_url = self.get_facebook_url(buttonset_soup_following)\n",
    "\n",
    "        urls_to_return = self.get_followers_or_following(\n",
    "            following_url,\n",
    "            html_of_following\n",
    "        )  # Scraping new urls\n",
    "        urls_to_return.extend(\n",
    "            self.get_followers_or_following(\n",
    "                followers_url,\n",
    "                html_of_followers\n",
    "            )\n",
    "        )\n",
    "        self.new_urls.extend(urls_to_return)\n",
    "\n",
    "        self.counter += 1\n",
    "        print(self.counter)\n",
    "        \n",
    "\n",
    "        user = dict(zip(\n",
    "                const.DEFAULT_FIELDS, \n",
    "                [\n",
    "                    hero_name,\n",
    "                    profile_url,\n",
    "                    hero_description,\n",
    "                    following_people_quantity,\n",
    "                    followers_quantity,\n",
    "                    twitter_url,\n",
    "                    facebook_url\n",
    "                ]\n",
    "        ))\n",
    "        self.write_to_csv(user)\n",
    "\n",
    "    def get_user_name(self, soup, current_url):\n",
    "        try:  # Scraping user's name\n",
    "            name = soup.find(const.H1_TAG).text\n",
    "        except AttributeError:\n",
    "            print(\"doesn't parse\")\n",
    "            return self.parse(current_url)\n",
    "        else:\n",
    "            return name\n",
    "\n",
    "    def get_user_description(self, soup):\n",
    "        try:  # Scraping user's description\n",
    "            hero_description = soup.find(\n",
    "                \"p\",\n",
    "                {\"class\": const.P_DESCRIPTION_CLASS}\n",
    "            ).text\n",
    "        except AttributeError:\n",
    "            return \"\"\n",
    "        else:\n",
    "            return hero_description\n",
    "\n",
    "    def get_number_of_following(self, soup):\n",
    "        try:  # Scraping number of following\n",
    "            following_people_quantity = int(\n",
    "                soup.find(\n",
    "                    \"a\",\n",
    "                    {\"data-action-value\": const.DATA_ACTION_FOLLOWING}\n",
    "                ).get(\"title\").split()[1].replace(\",\", \"\")\n",
    "            )\n",
    "        except AttributeError:\n",
    "            return None\n",
    "        else:\n",
    "            return following_people_quantity\n",
    "\n",
    "    def get_number_of_followers(self, soup):\n",
    "        try:  # Scraping number of followers\n",
    "            followers_quantity = int(\n",
    "                soup.find(\n",
    "                    \"a\",\n",
    "                    {\"data-action-value\": const.DATA_ACTION_FOLLOWERS}\n",
    "                ).get(\"title\").split()[1].replace(\",\", \"\")  \n",
    "            )\n",
    "        except AttributeError:\n",
    "            return None\n",
    "        else:\n",
    "            return followers_quantity\n",
    "\n",
    "    def get_twitter_url(self, soup):\n",
    "        try:  # Scraping twitter url\n",
    "            twitter = soup.find(\n",
    "                \"a\",\n",
    "                {\"title\": const.TITLE_TWITTER}\n",
    "            ).get(\"href\")\n",
    "        except AttributeError:\n",
    "            return None\n",
    "        else:\n",
    "            return twitter\n",
    "\n",
    "    def get_facebook_url(self, soup):\n",
    "        try:  # Scraping facebook url\n",
    "            facebook_url = soup.find(\n",
    "                \"a\",\n",
    "                {\"title\": const.TITLE_FACEBOOK}\n",
    "            ).get(\"href\")\n",
    "        except AttributeError:\n",
    "            return None\n",
    "        else:\n",
    "            return facebook_url\n",
    "\n",
    "    def write_to_csv(self, user):  # Write user to .csv table\n",
    "        with open(const.OUT_PATH, \"a\", encoding=const.CSV_ENCODING, newline=\"\") as csv_file:\n",
    "            field_names = const.DEFAULT_FIELDS\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=field_names)\n",
    "            writer.writerow(user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=App()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
